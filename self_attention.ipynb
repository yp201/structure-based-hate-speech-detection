{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('data/final_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0   0      3            0                   0        1      2   \n",
       "1           1   1      3            0                   1        0      1   \n",
       "2           2   2      3            0                   1        0      1   \n",
       "3           3   3      3            0                   1        0      1   \n",
       "4           4   4      6            0                   1        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(\"#\", \"\",tweet) # Removing '#' from hashtags\n",
    "    tweet = re.sub(\"RT\", \"\",tweet) # Removing 'RT' from hashtags\n",
    "    tweet = re.sub(\"[^a-zA-Z#]\", \" \",tweet) # Removing punctuation and special characters\n",
    "    tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "    tweet = re.sub('http','',tweet)\n",
    "    tweet = re.sub(\" +\", \" \", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = word_tokenize(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize=clean_tweet, lower=True,batch_first=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_datafields = [(\"\",None),(\"id\",None),(\"count\", None),\n",
    "                      (\"hate_speech\", LABEL),(\"offensive_language\", LABEL),\n",
    "                      (\"neither\", LABEL),(\"label\", None),(\"tweet\",TEXT)\n",
    "                    ]\n",
    "\n",
    "data = TabularDataset(\n",
    "        path=\"data/final_data.csv\", # the root directory where the data lies\n",
    "        format='csv',\n",
    "        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "        fields=dataset_datafields)\n",
    "train,test,valid=data.split([0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (train, valid), # we pass in the datasets we want the iterator to draw data from\n",
    "        batch_sizes=(64, 64),\n",
    "        device=device, # if you want to use the GPU, specify the GPU number here\n",
    "        sort_key=lambda x: len(x.tweet), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")\n",
    "test_iter = Iterator(test, batch_size=64, device=device, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = BatchWrapper(train_iter, \"tweet\", [\"hate_speech\",\"offensive_language\",\"neither\"])\n",
    "test_dl = BatchWrapper(test_iter, \"tweet\", [\"hate_speech\",\"offensive_language\",\"neither\"])\n",
    "val_dl = BatchWrapper(val_iter, \"tweet\", [\"hate_speech\",\"offensive_language\",\"neither\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL: Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        \n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.weights = weights\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.dropout = 0.8\n",
    "        self.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n",
    "        # We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n",
    "        self.W_s1 = nn.Linear(2*hidden_size, 350)\n",
    "        self.W_s2 = nn.Linear(350, 30)\n",
    "        self.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n",
    "        self.label = nn.Linear(2000, output_size)\n",
    "\n",
    "    def attention_net(self, lstm_output):\n",
    "\n",
    "        \"\"\"\n",
    "        Now we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n",
    "        encoding of the input sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n",
    "        the input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n",
    "        connected layer of size 2000 which will be connected to the output layer of size 2 returning logits for our two classes i.e., \n",
    "        pos & neg.\n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n",
    "        ---------\n",
    "        Returns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n",
    "                  attention to different parts of the input sentence.\n",
    "        Tensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n",
    "                      attn_weight_matrix.size() = (batch_size, 30, num_seq)\n",
    "        \"\"\"\n",
    "        attn_weight_matrix = self.W_s2(torch.tanh(self.W_s1(lstm_output)))\n",
    "        attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
    "        attn_weight_matrix = torch.softmax(attn_weight_matrix, dim=2)\n",
    "\n",
    "        return attn_weight_matrix\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "        output, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n",
    "        output = output.permute(1, 0, 2)\n",
    "\n",
    "        attn_weight_matrix = self.attention_net(output)\n",
    "        hidden_matrix = torch.bmm(attn_weight_matrix, output)\n",
    "\n",
    "        # Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n",
    "        fc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
    "        logits = self.label(fc_out)\n",
    "        # logits.size() = (batch_size, output_size)\n",
    "\n",
    "        return logits,attn_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashaswi.pathak/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (word_embeddings): Embedding(30359, 100)\n",
       "  (bilstm): LSTM(100, 500, dropout=0.8, bidirectional=True)\n",
       "  (W_s1): Linear(in_features=1000, out_features=350, bias=True)\n",
       "  (W_s2): Linear(in_features=350, out_features=30, bias=True)\n",
       "  (fc_layer): Linear(in_features=30000, out_features=2000, bias=True)\n",
       "  (label): Linear(in_features=2000, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=128\n",
    "num_classes=3\n",
    "h_emb=500\n",
    "emb_size=100\n",
    "vocab_Size=len(TEXT.vocab)\n",
    "vectors=TEXT.vocab.vectors\n",
    "model = SelfAttention(batch_size, num_classes, h_emb, vocab_Size, emb_size, TEXT.vocab.vectors)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',verbose=True,patience=5)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:09<00:00, 32.99it/s]\n",
      "  1%|          | 3/310 [00:00<00:11, 26.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.2770, Validation Loss: 0.2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:09<00:00, 32.34it/s]\n",
      "  1%|          | 3/310 [00:00<00:11, 26.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.1876, Validation Loss: 0.1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:09<00:00, 32.37it/s]\n",
      "  1%|          | 3/310 [00:00<00:11, 26.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.1588, Validation Loss: 0.2028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:09<00:00, 32.40it/s]\n",
      "  1%|          | 3/310 [00:00<00:11, 25.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.1281, Validation Loss: 0.2569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:09<00:00, 32.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.0974, Validation Loss: 0.3250\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x,y in tqdm.tqdm(train_dl):\n",
    "        text = x\n",
    "        target = y\n",
    "\n",
    "        opt.zero_grad()\n",
    "        prediction,attentin_matrix = model(text,text.size()[0])\n",
    "        loss = loss_func(prediction, target)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        opt.step()\n",
    "        running_loss += loss.data.item() * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x,y in val_dl:\n",
    "        preds,attentin_matrix = model(x,x.size()[0])\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(valid)\n",
    "    scheduler.step(val_loss)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=0\n",
    "count=0\n",
    "predictions=[]\n",
    "actual=[]\n",
    "for x,y in test_dl:\n",
    "    preds,attentin_matrix = model(x,x.size()[0])\n",
    "    preds = F.softmax(preds,1)\n",
    "    for idx,p in enumerate(preds):\n",
    "        curr_pred=np.argmax(np.round(p.detach().tolist()))\n",
    "        curr_gt=np.argmax(np.round(y[idx].tolist()))\n",
    "        predictions.append(curr_pred)\n",
    "        actual.append(curr_gt)\n",
    "        if np.array_equal(curr_pred,curr_gt):\n",
    "            accuracy+=1\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.8850342880193626\n",
      "Precision Score:  0.8850342880193627\n",
      "Recall Score:  0.8850342880193627\n",
      "Accuracy Score:  0.8850342880193627\n"
     ]
    }
   ],
   "source": [
    "f = f1_score( actual,predictions, average='micro')\n",
    "print(\"F1 Score: \", f)\n",
    "p = precision_score(actual, predictions, average='micro')\n",
    "print(\"Precision Score: \", p)\n",
    "r = recall_score(actual, predictions, average='micro')\n",
    "print(\"Recall Score: \", r)\n",
    "a = accuracy_score(actual, predictions)\n",
    "print(\"Accuracy Score: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of attention map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the attention matrix to analysze the individual contributions from the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  Hatespeech\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x153b5069c4a8>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n",
    "classes={0:'Hatespeech',1:'Offensive',2:'Neither'}\n",
    "test_sen1 = \"Mohammad is African\"\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "sentence=test_sen1\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "\n",
    "\n",
    "test_sen = np.asarray(test_sen1)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "\n",
    "output,attention_matrix = model(test_tensor, 1)\n",
    "out = torch.softmax(output, 1)\n",
    "print(\"Class: \", classes[torch.argmax(out).item()])\n",
    "\n",
    "attention=torch.mean(attention_matrix,1)\n",
    "attention = attention\n",
    "attended_words=zip(sentence,attention[0])\n",
    "individual_attention=list(np.zeros(len(sentence)))\n",
    "individual_labels=list(np.zeros(len(sentence)))\n",
    "for index,i in enumerate(attended_words):\n",
    "    individual_labels[index], individual_attention[index] = i[0], i[1].item()\n",
    "\n",
    "data_frame=pd.DataFrame(individual_attention,index=individual_labels)\n",
    "sns.heatmap(data_frame,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see sentence like \"Mohammad is African\" is predicted as hatespeech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  Neither\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x153b506e2208>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n",
    "classes={0:'Hatespeech',1:'Offensive',2:'Neither'}\n",
    "test_sen1 = \"Ram is Indian\"\n",
    "test_sen1 = TEXT.preprocess(test_sen1)\n",
    "sentence=test_sen1\n",
    "test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
    "\n",
    "\n",
    "test_sen = np.asarray(test_sen1)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen)\n",
    "test_tensor = test_tensor.cuda()\n",
    "model.eval()\n",
    "\n",
    "output,attention_matrix = model(test_tensor, 1)\n",
    "out = torch.softmax(output, 1)\n",
    "print(\"Class: \", classes[torch.argmax(out).item()])\n",
    "\n",
    "attention=torch.mean(attention_matrix,1)\n",
    "attention = attention\n",
    "attended_words=zip(sentence,attention[0])\n",
    "individual_attention=list(np.zeros(len(sentence)))\n",
    "individual_labels=list(np.zeros(len(sentence)))\n",
    "for index,i in enumerate(attended_words):\n",
    "    individual_labels[index], individual_attention[index] = i[0], i[1].item()\n",
    "\n",
    "data_frame=pd.DataFrame(individual_attention,index=individual_labels)\n",
    "sns.heatmap(data_frame,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see sentence like \"Ram is Indian\" is predicted as \"Neither\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above to results show the biasness in the model, these can be removed by using named entities ,like \"noun phase person\" inplace of actual name like \"ram\" and like wise for person.\n",
    "- One can also use a pretrained model to predict the score of these bias words from attention map, then take a reciprocal of these score and use these and ground truth for alpha_i in the attention map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
